{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.loadtxt(\"coding_test_dl_intern/mnist_train.csv\", delimiter=',')\n",
    "test_data = np.loadtxt(\"coding_test_dl_intern/mnist_test.csv\", delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_data[:, 0]\n",
    "train_images = train_data[:, 1:]\n",
    "\n",
    "X_train = train_images[(train_labels == 2) | (train_labels == 7)] / 255.\n",
    "\n",
    "y_train = train_labels[(train_labels == 2) | (train_labels == 7)]\n",
    "y_train[y_train == 2] = 0\n",
    "y_train[y_train == 7] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = test_data[:, 0]\n",
    "test_images = test_data[:, 1:]\n",
    "\n",
    "X_test = test_images[(test_labels == 2) | (test_labels == 7)] / 255.\n",
    "y_test = test_labels[(test_labels == 2) | (test_labels == 7)]\n",
    "y_test[y_test == 2] = 0\n",
    "y_test[y_test == 7] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Neural Networks implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(n_nodes, input_dim, seed=1749):\n",
    "    \"\"\"\n",
    "    Each row is a node, each column is a dimension of the input,\n",
    "    so dimension is (n_nodes, input_dim).\n",
    "    We initialize with a random normal in order to break symmetry.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    W = np.random.normal(size=(n_nodes, input_dim))\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_bias(n_nodes, seed=1749):\n",
    "    \"\"\"\n",
    "    The bias is added to Wx, therefore has dimension (n_nodes, 1).\n",
    "    Bias can be initialized to 0 since it isn't affected by the problem\n",
    "    of symmetry.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    b = np.zeros((n_nodes, 1))\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    This is the activation function. It takes as input\n",
    "    z = Wx + b, and applies the sigmoid function.\n",
    "    \"\"\"\n",
    "    return 1/(1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deriv_sigmoid(z):\n",
    "    \"\"\"\n",
    "    The derivation of the sigmoid function\n",
    "    \"\"\"\n",
    "    return sigmoid(z) * (1 - sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(y_hat, y, offset=True):\n",
    "    '''\n",
    "    Here we compute the loss function between two predictions.\n",
    "    If offset is true, we use a small value of epsilon to avoid numerical problems.\n",
    "    '''\n",
    "    if offset:\n",
    "        epsilon = 10e-30\n",
    "    else:\n",
    "        epsilon = 0\n",
    "        \n",
    "    loss = - (y * np.log(y_hat + epsilon) + (1-y) * np.log(1 - y_hat + epsilon))\n",
    "    return np.average(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FORWARD PROP\n",
    "def forward_prop(W1, b1, W2, b2, X, y):\n",
    "    '''\n",
    "    This process the forward propagation step\n",
    "    '''\n",
    "    # First hidden layer\n",
    "    Z1 = W1.dot(X) + b1\n",
    "    A1 = sigmoid(Z1)\n",
    "\n",
    "    # 2nd layer (output layer)\n",
    "    Z2 = W2.dot(A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    \n",
    "    return A1, A2, Z1, Z2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BACKPROP\n",
    "def backprop(A1, A2, W1, W2, Z1, Z2, X, y):\n",
    "    \"\"\"\n",
    "    This process the backward propagation algorithm.\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # Compute derivatives for second (output) layer\n",
    "    dZ2 = A2 - y\n",
    "    dW2 = (1/m) * dZ2.dot(A1.T)\n",
    "    db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "\n",
    "    # Computer derivatives for first layer\n",
    "    dZ1 = W2.T.dot(dZ2) * deriv_sigmoid(Z1)\n",
    "    dW1 = (1/m) * dZ1.dot(X.T)\n",
    "    db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    return dW1, dW2, db1, db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y, lr=0.001, n_iter=1000, display_training=True):\n",
    "    n_nodes = 300\n",
    "    \n",
    "    # Initialize the first layer\n",
    "    W1 = init_weights(n_nodes=n_nodes, input_dim=X.shape[0])\n",
    "    b1 = init_bias(n_nodes=n_nodes)\n",
    "\n",
    "    # Initialize the second layer\n",
    "    W2 = init_weights(n_nodes=1, input_dim=n_nodes)\n",
    "    b2 = init_bias(n_nodes=1)\n",
    "\n",
    "    # Run for n_iter iterations\n",
    "    for n in range(1, n_iter+1):\n",
    "        A1, A2, Z1, Z2 = forward_prop(W1, b1, W2, b2, X, y)\n",
    "        dW1, dW2, db1, db2 = backprop(A1, A2, W1, W2, Z1, Z2, X, y)\n",
    "\n",
    "        # Update all the layers\n",
    "        W1 = W1 - lr * dW1\n",
    "        W2 = W2 - lr * dW2\n",
    "        b1 = b1 - lr * db1\n",
    "        b2 = b2 - lr * db2\n",
    "\n",
    "        # Display loss\n",
    "        if n % 10 == 0 and display_training:\n",
    "            loss = compute_loss(A2, y)\n",
    "            print(f\"Iteration: {n}.\\t Loss: {loss}\")\n",
    "    \n",
    "    return W1, W2, b1, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, W1, W2, b1, b2):\n",
    "    # First hidden layer\n",
    "    Z1 = W1.dot(X) + b1\n",
    "    A1 = sigmoid(Z1)\n",
    "\n",
    "    # 2nd layer (output layer)\n",
    "    Z2 = W2.dot(A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    \n",
    "    y_proba = np.squeeze(A2)\n",
    "    y_pred = np.around(y_proba)\n",
    "    \n",
    "    return y_pred, y_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10.\t Loss: 6.713632961735607\n",
      "Iteration: 20.\t Loss: 6.437458379899252\n",
      "Iteration: 30.\t Loss: 6.171760037183202\n",
      "Iteration: 40.\t Loss: 5.917094214671635\n",
      "Iteration: 50.\t Loss: 5.6739369084114015\n",
      "Iteration: 60.\t Loss: 5.44269225762907\n",
      "Iteration: 70.\t Loss: 5.223684248138618\n",
      "Iteration: 80.\t Loss: 5.017127780658234\n",
      "Iteration: 90.\t Loss: 4.823092122567815\n",
      "Iteration: 100.\t Loss: 4.641478353974736\n",
      "Iteration: 110.\t Loss: 4.472022080326518\n",
      "Iteration: 120.\t Loss: 4.314317604484327\n",
      "Iteration: 130.\t Loss: 4.167852677303077\n",
      "Iteration: 140.\t Loss: 4.032042924745048\n",
      "Iteration: 150.\t Loss: 3.9062598426177857\n",
      "Iteration: 160.\t Loss: 3.789851786845553\n",
      "Iteration: 170.\t Loss: 3.682159769130984\n",
      "Iteration: 180.\t Loss: 3.5825298541361152\n",
      "Iteration: 190.\t Loss: 3.4903232891432743\n",
      "Iteration: 200.\t Loss: 3.404924703943308\n"
     ]
    }
   ],
   "source": [
    "W1, W2, b1, b2 = train(X_train.T, y_train, n_iter=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, y_proba = predict(X_test.T, W1, W2, b1, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 3.2195844421660076\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Loss:\", compute_loss(y_proba, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y):\n",
    "    return np.average(y_pred == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44854368932038835"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(y_pred, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
